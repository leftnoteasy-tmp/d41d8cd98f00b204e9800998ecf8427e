/* Generated by the protocol buffer compiler.  DO NOT EDIT! */

/* Do not generate deprecated warnings for self */
#ifndef PROTOBUF_C_NO_DEPRECATED
#define PROTOBUF_C_NO_DEPRECATED
#endif

#include "DatanodeProtocol.pb-c.h"
void   hadoop__hdfs__datanode_registration_proto__init
                     (Hadoop__Hdfs__DatanodeRegistrationProto         *message)
{
  static Hadoop__Hdfs__DatanodeRegistrationProto init_value = HADOOP__HDFS__DATANODE_REGISTRATION_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__datanode_registration_proto__get_packed_size
                     (const Hadoop__Hdfs__DatanodeRegistrationProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_registration_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__datanode_registration_proto__pack
                     (const Hadoop__Hdfs__DatanodeRegistrationProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_registration_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__datanode_registration_proto__pack_to_buffer
                     (const Hadoop__Hdfs__DatanodeRegistrationProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_registration_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__DatanodeRegistrationProto *
       hadoop__hdfs__datanode_registration_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__DatanodeRegistrationProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__datanode_registration_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__datanode_registration_proto__free_unpacked
                     (Hadoop__Hdfs__DatanodeRegistrationProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_registration_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__datanode_storage_proto__init
                     (Hadoop__Hdfs__DatanodeStorageProto         *message)
{
  static Hadoop__Hdfs__DatanodeStorageProto init_value = HADOOP__HDFS__DATANODE_STORAGE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__datanode_storage_proto__get_packed_size
                     (const Hadoop__Hdfs__DatanodeStorageProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_storage_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__datanode_storage_proto__pack
                     (const Hadoop__Hdfs__DatanodeStorageProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_storage_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__datanode_storage_proto__pack_to_buffer
                     (const Hadoop__Hdfs__DatanodeStorageProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_storage_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__DatanodeStorageProto *
       hadoop__hdfs__datanode_storage_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__DatanodeStorageProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__datanode_storage_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__datanode_storage_proto__free_unpacked
                     (Hadoop__Hdfs__DatanodeStorageProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_storage_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__datanode_command_proto__init
                     (Hadoop__Hdfs__DatanodeCommandProto         *message)
{
  static Hadoop__Hdfs__DatanodeCommandProto init_value = HADOOP__HDFS__DATANODE_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__datanode_command_proto__get_packed_size
                     (const Hadoop__Hdfs__DatanodeCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__datanode_command_proto__pack
                     (const Hadoop__Hdfs__DatanodeCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__datanode_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__DatanodeCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__DatanodeCommandProto *
       hadoop__hdfs__datanode_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__DatanodeCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__datanode_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__datanode_command_proto__free_unpacked
                     (Hadoop__Hdfs__DatanodeCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__datanode_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__balancer_bandwidth_command_proto__init
                     (Hadoop__Hdfs__BalancerBandwidthCommandProto         *message)
{
  static Hadoop__Hdfs__BalancerBandwidthCommandProto init_value = HADOOP__HDFS__BALANCER_BANDWIDTH_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__balancer_bandwidth_command_proto__get_packed_size
                     (const Hadoop__Hdfs__BalancerBandwidthCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__balancer_bandwidth_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__balancer_bandwidth_command_proto__pack
                     (const Hadoop__Hdfs__BalancerBandwidthCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__balancer_bandwidth_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__balancer_bandwidth_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BalancerBandwidthCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__balancer_bandwidth_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BalancerBandwidthCommandProto *
       hadoop__hdfs__balancer_bandwidth_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BalancerBandwidthCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__balancer_bandwidth_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__balancer_bandwidth_command_proto__free_unpacked
                     (Hadoop__Hdfs__BalancerBandwidthCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__balancer_bandwidth_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_command_proto__init
                     (Hadoop__Hdfs__BlockCommandProto         *message)
{
  static Hadoop__Hdfs__BlockCommandProto init_value = HADOOP__HDFS__BLOCK_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_command_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_command_proto__pack
                     (const Hadoop__Hdfs__BlockCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockCommandProto *
       hadoop__hdfs__block_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_command_proto__free_unpacked
                     (Hadoop__Hdfs__BlockCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_recovery_command_proto__init
                     (Hadoop__Hdfs__BlockRecoveryCommandProto         *message)
{
  static Hadoop__Hdfs__BlockRecoveryCommandProto init_value = HADOOP__HDFS__BLOCK_RECOVERY_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_recovery_command_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockRecoveryCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_recovery_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_recovery_command_proto__pack
                     (const Hadoop__Hdfs__BlockRecoveryCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_recovery_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_recovery_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockRecoveryCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_recovery_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockRecoveryCommandProto *
       hadoop__hdfs__block_recovery_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockRecoveryCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_recovery_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_recovery_command_proto__free_unpacked
                     (Hadoop__Hdfs__BlockRecoveryCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_recovery_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__finalize_command_proto__init
                     (Hadoop__Hdfs__FinalizeCommandProto         *message)
{
  static Hadoop__Hdfs__FinalizeCommandProto init_value = HADOOP__HDFS__FINALIZE_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__finalize_command_proto__get_packed_size
                     (const Hadoop__Hdfs__FinalizeCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__finalize_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__finalize_command_proto__pack
                     (const Hadoop__Hdfs__FinalizeCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__finalize_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__finalize_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__FinalizeCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__finalize_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__FinalizeCommandProto *
       hadoop__hdfs__finalize_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__FinalizeCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__finalize_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__finalize_command_proto__free_unpacked
                     (Hadoop__Hdfs__FinalizeCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__finalize_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__key_update_command_proto__init
                     (Hadoop__Hdfs__KeyUpdateCommandProto         *message)
{
  static Hadoop__Hdfs__KeyUpdateCommandProto init_value = HADOOP__HDFS__KEY_UPDATE_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__key_update_command_proto__get_packed_size
                     (const Hadoop__Hdfs__KeyUpdateCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__key_update_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__key_update_command_proto__pack
                     (const Hadoop__Hdfs__KeyUpdateCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__key_update_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__key_update_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__KeyUpdateCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__key_update_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__KeyUpdateCommandProto *
       hadoop__hdfs__key_update_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__KeyUpdateCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__key_update_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__key_update_command_proto__free_unpacked
                     (Hadoop__Hdfs__KeyUpdateCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__key_update_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__register_command_proto__init
                     (Hadoop__Hdfs__RegisterCommandProto         *message)
{
  static Hadoop__Hdfs__RegisterCommandProto init_value = HADOOP__HDFS__REGISTER_COMMAND_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__register_command_proto__get_packed_size
                     (const Hadoop__Hdfs__RegisterCommandProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_command_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__register_command_proto__pack
                     (const Hadoop__Hdfs__RegisterCommandProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_command_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__register_command_proto__pack_to_buffer
                     (const Hadoop__Hdfs__RegisterCommandProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_command_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__RegisterCommandProto *
       hadoop__hdfs__register_command_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__RegisterCommandProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__register_command_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__register_command_proto__free_unpacked
                     (Hadoop__Hdfs__RegisterCommandProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_command_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__register_datanode_request_proto__init
                     (Hadoop__Hdfs__RegisterDatanodeRequestProto         *message)
{
  static Hadoop__Hdfs__RegisterDatanodeRequestProto init_value = HADOOP__HDFS__REGISTER_DATANODE_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__register_datanode_request_proto__get_packed_size
                     (const Hadoop__Hdfs__RegisterDatanodeRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__register_datanode_request_proto__pack
                     (const Hadoop__Hdfs__RegisterDatanodeRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__register_datanode_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__RegisterDatanodeRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__RegisterDatanodeRequestProto *
       hadoop__hdfs__register_datanode_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__RegisterDatanodeRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__register_datanode_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__register_datanode_request_proto__free_unpacked
                     (Hadoop__Hdfs__RegisterDatanodeRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__register_datanode_response_proto__init
                     (Hadoop__Hdfs__RegisterDatanodeResponseProto         *message)
{
  static Hadoop__Hdfs__RegisterDatanodeResponseProto init_value = HADOOP__HDFS__REGISTER_DATANODE_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__register_datanode_response_proto__get_packed_size
                     (const Hadoop__Hdfs__RegisterDatanodeResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__register_datanode_response_proto__pack
                     (const Hadoop__Hdfs__RegisterDatanodeResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__register_datanode_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__RegisterDatanodeResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__RegisterDatanodeResponseProto *
       hadoop__hdfs__register_datanode_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__RegisterDatanodeResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__register_datanode_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__register_datanode_response_proto__free_unpacked
                     (Hadoop__Hdfs__RegisterDatanodeResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__register_datanode_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__heartbeat_request_proto__init
                     (Hadoop__Hdfs__HeartbeatRequestProto         *message)
{
  static Hadoop__Hdfs__HeartbeatRequestProto init_value = HADOOP__HDFS__HEARTBEAT_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__heartbeat_request_proto__get_packed_size
                     (const Hadoop__Hdfs__HeartbeatRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__heartbeat_request_proto__pack
                     (const Hadoop__Hdfs__HeartbeatRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__heartbeat_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__HeartbeatRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__HeartbeatRequestProto *
       hadoop__hdfs__heartbeat_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__HeartbeatRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__heartbeat_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__heartbeat_request_proto__free_unpacked
                     (Hadoop__Hdfs__HeartbeatRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__storage_report_proto__init
                     (Hadoop__Hdfs__StorageReportProto         *message)
{
  static Hadoop__Hdfs__StorageReportProto init_value = HADOOP__HDFS__STORAGE_REPORT_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__storage_report_proto__get_packed_size
                     (const Hadoop__Hdfs__StorageReportProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_report_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__storage_report_proto__pack
                     (const Hadoop__Hdfs__StorageReportProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_report_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__storage_report_proto__pack_to_buffer
                     (const Hadoop__Hdfs__StorageReportProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_report_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__StorageReportProto *
       hadoop__hdfs__storage_report_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__StorageReportProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__storage_report_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__storage_report_proto__free_unpacked
                     (Hadoop__Hdfs__StorageReportProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_report_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__nnhastatus_heartbeat_proto__init
                     (Hadoop__Hdfs__NNHAStatusHeartbeatProto         *message)
{
  static Hadoop__Hdfs__NNHAStatusHeartbeatProto init_value = HADOOP__HDFS__NNHASTATUS_HEARTBEAT_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__nnhastatus_heartbeat_proto__get_packed_size
                     (const Hadoop__Hdfs__NNHAStatusHeartbeatProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__nnhastatus_heartbeat_proto__pack
                     (const Hadoop__Hdfs__NNHAStatusHeartbeatProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__nnhastatus_heartbeat_proto__pack_to_buffer
                     (const Hadoop__Hdfs__NNHAStatusHeartbeatProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__NNHAStatusHeartbeatProto *
       hadoop__hdfs__nnhastatus_heartbeat_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__NNHAStatusHeartbeatProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__nnhastatus_heartbeat_proto__free_unpacked
                     (Hadoop__Hdfs__NNHAStatusHeartbeatProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__heartbeat_response_proto__init
                     (Hadoop__Hdfs__HeartbeatResponseProto         *message)
{
  static Hadoop__Hdfs__HeartbeatResponseProto init_value = HADOOP__HDFS__HEARTBEAT_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__heartbeat_response_proto__get_packed_size
                     (const Hadoop__Hdfs__HeartbeatResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__heartbeat_response_proto__pack
                     (const Hadoop__Hdfs__HeartbeatResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__heartbeat_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__HeartbeatResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__HeartbeatResponseProto *
       hadoop__hdfs__heartbeat_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__HeartbeatResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__heartbeat_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__heartbeat_response_proto__free_unpacked
                     (Hadoop__Hdfs__HeartbeatResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__heartbeat_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_report_request_proto__init
                     (Hadoop__Hdfs__BlockReportRequestProto         *message)
{
  static Hadoop__Hdfs__BlockReportRequestProto init_value = HADOOP__HDFS__BLOCK_REPORT_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_report_request_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockReportRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_report_request_proto__pack
                     (const Hadoop__Hdfs__BlockReportRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_report_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockReportRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockReportRequestProto *
       hadoop__hdfs__block_report_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockReportRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_report_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_report_request_proto__free_unpacked
                     (Hadoop__Hdfs__BlockReportRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__storage_block_report_proto__init
                     (Hadoop__Hdfs__StorageBlockReportProto         *message)
{
  static Hadoop__Hdfs__StorageBlockReportProto init_value = HADOOP__HDFS__STORAGE_BLOCK_REPORT_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__storage_block_report_proto__get_packed_size
                     (const Hadoop__Hdfs__StorageBlockReportProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_block_report_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__storage_block_report_proto__pack
                     (const Hadoop__Hdfs__StorageBlockReportProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_block_report_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__storage_block_report_proto__pack_to_buffer
                     (const Hadoop__Hdfs__StorageBlockReportProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_block_report_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__StorageBlockReportProto *
       hadoop__hdfs__storage_block_report_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__StorageBlockReportProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__storage_block_report_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__storage_block_report_proto__free_unpacked
                     (Hadoop__Hdfs__StorageBlockReportProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_block_report_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_report_response_proto__init
                     (Hadoop__Hdfs__BlockReportResponseProto         *message)
{
  static Hadoop__Hdfs__BlockReportResponseProto init_value = HADOOP__HDFS__BLOCK_REPORT_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_report_response_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockReportResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_report_response_proto__pack
                     (const Hadoop__Hdfs__BlockReportResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_report_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockReportResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockReportResponseProto *
       hadoop__hdfs__block_report_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockReportResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_report_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_report_response_proto__free_unpacked
                     (Hadoop__Hdfs__BlockReportResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_report_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__received_deleted_block_info_proto__init
                     (Hadoop__Hdfs__ReceivedDeletedBlockInfoProto         *message)
{
  static Hadoop__Hdfs__ReceivedDeletedBlockInfoProto init_value = HADOOP__HDFS__RECEIVED_DELETED_BLOCK_INFO_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__received_deleted_block_info_proto__get_packed_size
                     (const Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__received_deleted_block_info_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__received_deleted_block_info_proto__pack
                     (const Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__received_deleted_block_info_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__received_deleted_block_info_proto__pack_to_buffer
                     (const Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__received_deleted_block_info_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *
       hadoop__hdfs__received_deleted_block_info_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__received_deleted_block_info_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__received_deleted_block_info_proto__free_unpacked
                     (Hadoop__Hdfs__ReceivedDeletedBlockInfoProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__received_deleted_block_info_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__storage_received_deleted_blocks_proto__init
                     (Hadoop__Hdfs__StorageReceivedDeletedBlocksProto         *message)
{
  static Hadoop__Hdfs__StorageReceivedDeletedBlocksProto init_value = HADOOP__HDFS__STORAGE_RECEIVED_DELETED_BLOCKS_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__storage_received_deleted_blocks_proto__get_packed_size
                     (const Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__storage_received_deleted_blocks_proto__pack
                     (const Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__storage_received_deleted_blocks_proto__pack_to_buffer
                     (const Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *
       hadoop__hdfs__storage_received_deleted_blocks_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__storage_received_deleted_blocks_proto__free_unpacked
                     (Hadoop__Hdfs__StorageReceivedDeletedBlocksProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_received_and_deleted_request_proto__init
                     (Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto         *message)
{
  static Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto init_value = HADOOP__HDFS__BLOCK_RECEIVED_AND_DELETED_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_received_and_deleted_request_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_received_and_deleted_request_proto__pack
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_received_and_deleted_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *
       hadoop__hdfs__block_received_and_deleted_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_received_and_deleted_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_received_and_deleted_request_proto__free_unpacked
                     (Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__block_received_and_deleted_response_proto__init
                     (Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto         *message)
{
  static Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto init_value = HADOOP__HDFS__BLOCK_RECEIVED_AND_DELETED_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__block_received_and_deleted_response_proto__get_packed_size
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__block_received_and_deleted_response_proto__pack
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__block_received_and_deleted_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *
       hadoop__hdfs__block_received_and_deleted_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__block_received_and_deleted_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__block_received_and_deleted_response_proto__free_unpacked
                     (Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__block_received_and_deleted_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__error_report_request_proto__init
                     (Hadoop__Hdfs__ErrorReportRequestProto         *message)
{
  static Hadoop__Hdfs__ErrorReportRequestProto init_value = HADOOP__HDFS__ERROR_REPORT_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__error_report_request_proto__get_packed_size
                     (const Hadoop__Hdfs__ErrorReportRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__error_report_request_proto__pack
                     (const Hadoop__Hdfs__ErrorReportRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__error_report_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__ErrorReportRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__ErrorReportRequestProto *
       hadoop__hdfs__error_report_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__ErrorReportRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__error_report_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__error_report_request_proto__free_unpacked
                     (Hadoop__Hdfs__ErrorReportRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__error_report_response_proto__init
                     (Hadoop__Hdfs__ErrorReportResponseProto         *message)
{
  static Hadoop__Hdfs__ErrorReportResponseProto init_value = HADOOP__HDFS__ERROR_REPORT_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__error_report_response_proto__get_packed_size
                     (const Hadoop__Hdfs__ErrorReportResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__error_report_response_proto__pack
                     (const Hadoop__Hdfs__ErrorReportResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__error_report_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__ErrorReportResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__ErrorReportResponseProto *
       hadoop__hdfs__error_report_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__ErrorReportResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__error_report_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__error_report_response_proto__free_unpacked
                     (Hadoop__Hdfs__ErrorReportResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__error_report_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__report_bad_blocks_request_proto__init
                     (Hadoop__Hdfs__ReportBadBlocksRequestProto         *message)
{
  static Hadoop__Hdfs__ReportBadBlocksRequestProto init_value = HADOOP__HDFS__REPORT_BAD_BLOCKS_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__report_bad_blocks_request_proto__get_packed_size
                     (const Hadoop__Hdfs__ReportBadBlocksRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__report_bad_blocks_request_proto__pack
                     (const Hadoop__Hdfs__ReportBadBlocksRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__report_bad_blocks_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__ReportBadBlocksRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__ReportBadBlocksRequestProto *
       hadoop__hdfs__report_bad_blocks_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__ReportBadBlocksRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__report_bad_blocks_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__report_bad_blocks_request_proto__free_unpacked
                     (Hadoop__Hdfs__ReportBadBlocksRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__report_bad_blocks_response_proto__init
                     (Hadoop__Hdfs__ReportBadBlocksResponseProto         *message)
{
  static Hadoop__Hdfs__ReportBadBlocksResponseProto init_value = HADOOP__HDFS__REPORT_BAD_BLOCKS_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__report_bad_blocks_response_proto__get_packed_size
                     (const Hadoop__Hdfs__ReportBadBlocksResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__report_bad_blocks_response_proto__pack
                     (const Hadoop__Hdfs__ReportBadBlocksResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__report_bad_blocks_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__ReportBadBlocksResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__ReportBadBlocksResponseProto *
       hadoop__hdfs__report_bad_blocks_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__ReportBadBlocksResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__report_bad_blocks_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__report_bad_blocks_response_proto__free_unpacked
                     (Hadoop__Hdfs__ReportBadBlocksResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__report_bad_blocks_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__commit_block_synchronization_request_proto__init
                     (Hadoop__Hdfs__CommitBlockSynchronizationRequestProto         *message)
{
  static Hadoop__Hdfs__CommitBlockSynchronizationRequestProto init_value = HADOOP__HDFS__COMMIT_BLOCK_SYNCHRONIZATION_REQUEST_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__commit_block_synchronization_request_proto__get_packed_size
                     (const Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_request_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__commit_block_synchronization_request_proto__pack
                     (const Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_request_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__commit_block_synchronization_request_proto__pack_to_buffer
                     (const Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_request_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *
       hadoop__hdfs__commit_block_synchronization_request_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__commit_block_synchronization_request_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__commit_block_synchronization_request_proto__free_unpacked
                     (Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_request_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
void   hadoop__hdfs__commit_block_synchronization_response_proto__init
                     (Hadoop__Hdfs__CommitBlockSynchronizationResponseProto         *message)
{
  static Hadoop__Hdfs__CommitBlockSynchronizationResponseProto init_value = HADOOP__HDFS__COMMIT_BLOCK_SYNCHRONIZATION_RESPONSE_PROTO__INIT;
  *message = init_value;
}
size_t hadoop__hdfs__commit_block_synchronization_response_proto__get_packed_size
                     (const Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *message)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_response_proto__descriptor);
  return protobuf_c_message_get_packed_size ((const ProtobufCMessage*)(message));
}
size_t hadoop__hdfs__commit_block_synchronization_response_proto__pack
                     (const Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *message,
                      uint8_t       *out)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_response_proto__descriptor);
  return protobuf_c_message_pack ((const ProtobufCMessage*)message, out);
}
size_t hadoop__hdfs__commit_block_synchronization_response_proto__pack_to_buffer
                     (const Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *message,
                      ProtobufCBuffer *buffer)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_response_proto__descriptor);
  return protobuf_c_message_pack_to_buffer ((const ProtobufCMessage*)message, buffer);
}
Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *
       hadoop__hdfs__commit_block_synchronization_response_proto__unpack
                     (ProtobufCAllocator  *allocator,
                      size_t               len,
                      const uint8_t       *data)
{
  return (Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *)
     protobuf_c_message_unpack (&hadoop__hdfs__commit_block_synchronization_response_proto__descriptor,
                                allocator, len, data);
}
void   hadoop__hdfs__commit_block_synchronization_response_proto__free_unpacked
                     (Hadoop__Hdfs__CommitBlockSynchronizationResponseProto *message,
                      ProtobufCAllocator *allocator)
{
  PROTOBUF_C_ASSERT (message->base.descriptor == &hadoop__hdfs__commit_block_synchronization_response_proto__descriptor);
  protobuf_c_message_free_unpacked ((ProtobufCMessage*)message, allocator);
}
static const ProtobufCFieldDescriptor hadoop__hdfs__datanode_registration_proto__field_descriptors[4] =
{
  {
    "datanodeID",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeRegistrationProto, datanodeid),
    &hadoop__hdfs__datanode_idproto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "storageInfo",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeRegistrationProto, storageinfo),
    &hadoop__hdfs__storage_info_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "keys",
    3,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeRegistrationProto, keys),
    &hadoop__hdfs__exported_block_keys_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "softwareVersion",
    4,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeRegistrationProto, softwareversion),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__datanode_registration_proto__field_indices_by_name[] = {
  0,   /* field[0] = datanodeID */
  2,   /* field[2] = keys */
  3,   /* field[3] = softwareVersion */
  1,   /* field[1] = storageInfo */
};
static const ProtobufCIntRange hadoop__hdfs__datanode_registration_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 4 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__datanode_registration_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeRegistrationProto",
  "DatanodeRegistrationProto",
  "Hadoop__Hdfs__DatanodeRegistrationProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__DatanodeRegistrationProto),
  4,
  hadoop__hdfs__datanode_registration_proto__field_descriptors,
  hadoop__hdfs__datanode_registration_proto__field_indices_by_name,
  1,  hadoop__hdfs__datanode_registration_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__datanode_registration_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__datanode_storage_proto__storage_state__enum_values_by_number[2] =
{
  { "NORMAL", "HADOOP__HDFS__DATANODE_STORAGE_PROTO__STORAGE_STATE__NORMAL", 0 },
  { "READ_ONLY", "HADOOP__HDFS__DATANODE_STORAGE_PROTO__STORAGE_STATE__READ_ONLY", 1 },
};
static const ProtobufCIntRange hadoop__hdfs__datanode_storage_proto__storage_state__value_ranges[] = {
{0, 0},{0, 2}
};
const ProtobufCEnumValueIndex hadoop__hdfs__datanode_storage_proto__storage_state__enum_values_by_name[2] =
{
  { "NORMAL", 0 },
  { "READ_ONLY", 1 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__datanode_storage_proto__storage_state__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeStorageProto.StorageState",
  "StorageState",
  "Hadoop__Hdfs__DatanodeStorageProto__StorageState",
  "hadoop.hdfs",
  2,
  hadoop__hdfs__datanode_storage_proto__storage_state__enum_values_by_number,
  2,
  hadoop__hdfs__datanode_storage_proto__storage_state__enum_values_by_name,
  1,
  hadoop__hdfs__datanode_storage_proto__storage_state__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const Hadoop__Hdfs__DatanodeStorageProto__StorageState hadoop__hdfs__datanode_storage_proto__state__default_value = HADOOP__HDFS__DATANODE_STORAGE_PROTO__STORAGE_STATE__NORMAL;
static const ProtobufCFieldDescriptor hadoop__hdfs__datanode_storage_proto__field_descriptors[2] =
{
  {
    "storageID",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeStorageProto, storageid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "state",
    2,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_ENUM,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeStorageProto, has_state),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeStorageProto, state),
    &hadoop__hdfs__datanode_storage_proto__storage_state__descriptor,
    &hadoop__hdfs__datanode_storage_proto__state__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__datanode_storage_proto__field_indices_by_name[] = {
  1,   /* field[1] = state */
  0,   /* field[0] = storageID */
};
static const ProtobufCIntRange hadoop__hdfs__datanode_storage_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__datanode_storage_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeStorageProto",
  "DatanodeStorageProto",
  "Hadoop__Hdfs__DatanodeStorageProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__DatanodeStorageProto),
  2,
  hadoop__hdfs__datanode_storage_proto__field_descriptors,
  hadoop__hdfs__datanode_storage_proto__field_indices_by_name,
  1,  hadoop__hdfs__datanode_storage_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__datanode_storage_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__datanode_command_proto__type__enum_values_by_number[8] =
{
  { "BalancerBandwidthCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__BALANCERBANDWIDTHCOMMAND", 0 },
  { "BlockCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__BLOCKCOMMAND", 1 },
  { "BlockRecoveryCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__BLOCKRECOVERYCOMMAND", 2 },
  { "FinalizeCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__FINALIZECOMMAND", 3 },
  { "KeyUpdateCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__KEYUPDATECOMMAND", 4 },
  { "RegisterCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__REGISTERCOMMAND", 5 },
  { "UnusedUpgradeCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__UNUSEDUPGRADECOMMAND", 6 },
  { "NullDatanodeCommand", "HADOOP__HDFS__DATANODE_COMMAND_PROTO__TYPE__NULLDATANODECOMMAND", 7 },
};
static const ProtobufCIntRange hadoop__hdfs__datanode_command_proto__type__value_ranges[] = {
{0, 0},{0, 8}
};
const ProtobufCEnumValueIndex hadoop__hdfs__datanode_command_proto__type__enum_values_by_name[8] =
{
  { "BalancerBandwidthCommand", 0 },
  { "BlockCommand", 1 },
  { "BlockRecoveryCommand", 2 },
  { "FinalizeCommand", 3 },
  { "KeyUpdateCommand", 4 },
  { "NullDatanodeCommand", 7 },
  { "RegisterCommand", 5 },
  { "UnusedUpgradeCommand", 6 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__datanode_command_proto__type__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeCommandProto.Type",
  "Type",
  "Hadoop__Hdfs__DatanodeCommandProto__Type",
  "hadoop.hdfs",
  8,
  hadoop__hdfs__datanode_command_proto__type__enum_values_by_number,
  8,
  hadoop__hdfs__datanode_command_proto__type__enum_values_by_name,
  1,
  hadoop__hdfs__datanode_command_proto__type__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__datanode_command_proto__field_descriptors[7] =
{
  {
    "cmdType",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_ENUM,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, cmdtype),
    &hadoop__hdfs__datanode_command_proto__type__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "balancerCmd",
    2,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, balancercmd),
    &hadoop__hdfs__balancer_bandwidth_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blkCmd",
    3,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, blkcmd),
    &hadoop__hdfs__block_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "recoveryCmd",
    4,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, recoverycmd),
    &hadoop__hdfs__block_recovery_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "finalizeCmd",
    5,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, finalizecmd),
    &hadoop__hdfs__finalize_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "keyUpdateCmd",
    6,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, keyupdatecmd),
    &hadoop__hdfs__key_update_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "registerCmd",
    7,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__DatanodeCommandProto, registercmd),
    &hadoop__hdfs__register_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__datanode_command_proto__field_indices_by_name[] = {
  1,   /* field[1] = balancerCmd */
  2,   /* field[2] = blkCmd */
  0,   /* field[0] = cmdType */
  4,   /* field[4] = finalizeCmd */
  5,   /* field[5] = keyUpdateCmd */
  3,   /* field[3] = recoveryCmd */
  6,   /* field[6] = registerCmd */
};
static const ProtobufCIntRange hadoop__hdfs__datanode_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 7 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__datanode_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeCommandProto",
  "DatanodeCommandProto",
  "Hadoop__Hdfs__DatanodeCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__DatanodeCommandProto),
  7,
  hadoop__hdfs__datanode_command_proto__field_descriptors,
  hadoop__hdfs__datanode_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__datanode_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__datanode_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__balancer_bandwidth_command_proto__field_descriptors[1] =
{
  {
    "bandwidth",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_UINT64,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BalancerBandwidthCommandProto, bandwidth),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__balancer_bandwidth_command_proto__field_indices_by_name[] = {
  0,   /* field[0] = bandwidth */
};
static const ProtobufCIntRange hadoop__hdfs__balancer_bandwidth_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__balancer_bandwidth_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BalancerBandwidthCommandProto",
  "BalancerBandwidthCommandProto",
  "Hadoop__Hdfs__BalancerBandwidthCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BalancerBandwidthCommandProto),
  1,
  hadoop__hdfs__balancer_bandwidth_command_proto__field_descriptors,
  hadoop__hdfs__balancer_bandwidth_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__balancer_bandwidth_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__balancer_bandwidth_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__block_command_proto__action__enum_values_by_number[3] =
{
  { "TRANSFER", "HADOOP__HDFS__BLOCK_COMMAND_PROTO__ACTION__TRANSFER", 1 },
  { "INVALIDATE", "HADOOP__HDFS__BLOCK_COMMAND_PROTO__ACTION__INVALIDATE", 2 },
  { "SHUTDOWN", "HADOOP__HDFS__BLOCK_COMMAND_PROTO__ACTION__SHUTDOWN", 3 },
};
static const ProtobufCIntRange hadoop__hdfs__block_command_proto__action__value_ranges[] = {
{1, 0},{0, 3}
};
const ProtobufCEnumValueIndex hadoop__hdfs__block_command_proto__action__enum_values_by_name[3] =
{
  { "INVALIDATE", 1 },
  { "SHUTDOWN", 2 },
  { "TRANSFER", 0 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__block_command_proto__action__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockCommandProto.Action",
  "Action",
  "Hadoop__Hdfs__BlockCommandProto__Action",
  "hadoop.hdfs",
  3,
  hadoop__hdfs__block_command_proto__action__enum_values_by_number,
  3,
  hadoop__hdfs__block_command_proto__action__enum_values_by_name,
  1,
  hadoop__hdfs__block_command_proto__action__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__block_command_proto__field_descriptors[4] =
{
  {
    "action",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_ENUM,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, action),
    &hadoop__hdfs__block_command_proto__action__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blockPoolId",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, blockpoolid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blocks",
    3,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, blocks),
    &hadoop__hdfs__block_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "targets",
    4,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, n_targets),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockCommandProto, targets),
    &hadoop__hdfs__datanode_infos_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__block_command_proto__field_indices_by_name[] = {
  0,   /* field[0] = action */
  1,   /* field[1] = blockPoolId */
  2,   /* field[2] = blocks */
  3,   /* field[3] = targets */
};
static const ProtobufCIntRange hadoop__hdfs__block_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 4 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__block_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockCommandProto",
  "BlockCommandProto",
  "Hadoop__Hdfs__BlockCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockCommandProto),
  4,
  hadoop__hdfs__block_command_proto__field_descriptors,
  hadoop__hdfs__block_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__block_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__block_recovery_command_proto__field_descriptors[1] =
{
  {
    "blocks",
    1,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockRecoveryCommandProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockRecoveryCommandProto, blocks),
    &hadoop__hdfs__recovering_block_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__block_recovery_command_proto__field_indices_by_name[] = {
  0,   /* field[0] = blocks */
};
static const ProtobufCIntRange hadoop__hdfs__block_recovery_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__block_recovery_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockRecoveryCommandProto",
  "BlockRecoveryCommandProto",
  "Hadoop__Hdfs__BlockRecoveryCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockRecoveryCommandProto),
  1,
  hadoop__hdfs__block_recovery_command_proto__field_descriptors,
  hadoop__hdfs__block_recovery_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__block_recovery_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_recovery_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__finalize_command_proto__field_descriptors[1] =
{
  {
    "blockPoolId",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__FinalizeCommandProto, blockpoolid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__finalize_command_proto__field_indices_by_name[] = {
  0,   /* field[0] = blockPoolId */
};
static const ProtobufCIntRange hadoop__hdfs__finalize_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__finalize_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.FinalizeCommandProto",
  "FinalizeCommandProto",
  "Hadoop__Hdfs__FinalizeCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__FinalizeCommandProto),
  1,
  hadoop__hdfs__finalize_command_proto__field_descriptors,
  hadoop__hdfs__finalize_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__finalize_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__finalize_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__key_update_command_proto__field_descriptors[1] =
{
  {
    "keys",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__KeyUpdateCommandProto, keys),
    &hadoop__hdfs__exported_block_keys_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__key_update_command_proto__field_indices_by_name[] = {
  0,   /* field[0] = keys */
};
static const ProtobufCIntRange hadoop__hdfs__key_update_command_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__key_update_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.KeyUpdateCommandProto",
  "KeyUpdateCommandProto",
  "Hadoop__Hdfs__KeyUpdateCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__KeyUpdateCommandProto),
  1,
  hadoop__hdfs__key_update_command_proto__field_descriptors,
  hadoop__hdfs__key_update_command_proto__field_indices_by_name,
  1,  hadoop__hdfs__key_update_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__key_update_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
#define hadoop__hdfs__register_command_proto__field_descriptors NULL
#define hadoop__hdfs__register_command_proto__field_indices_by_name NULL
#define hadoop__hdfs__register_command_proto__number_ranges NULL
const ProtobufCMessageDescriptor hadoop__hdfs__register_command_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.RegisterCommandProto",
  "RegisterCommandProto",
  "Hadoop__Hdfs__RegisterCommandProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__RegisterCommandProto),
  0,
  hadoop__hdfs__register_command_proto__field_descriptors,
  hadoop__hdfs__register_command_proto__field_indices_by_name,
  0,  hadoop__hdfs__register_command_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__register_command_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__register_datanode_request_proto__field_descriptors[1] =
{
  {
    "registration",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__RegisterDatanodeRequestProto, registration),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__register_datanode_request_proto__field_indices_by_name[] = {
  0,   /* field[0] = registration */
};
static const ProtobufCIntRange hadoop__hdfs__register_datanode_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__register_datanode_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.RegisterDatanodeRequestProto",
  "RegisterDatanodeRequestProto",
  "Hadoop__Hdfs__RegisterDatanodeRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__RegisterDatanodeRequestProto),
  1,
  hadoop__hdfs__register_datanode_request_proto__field_descriptors,
  hadoop__hdfs__register_datanode_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__register_datanode_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__register_datanode_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__register_datanode_response_proto__field_descriptors[1] =
{
  {
    "registration",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__RegisterDatanodeResponseProto, registration),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__register_datanode_response_proto__field_indices_by_name[] = {
  0,   /* field[0] = registration */
};
static const ProtobufCIntRange hadoop__hdfs__register_datanode_response_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__register_datanode_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.RegisterDatanodeResponseProto",
  "RegisterDatanodeResponseProto",
  "Hadoop__Hdfs__RegisterDatanodeResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__RegisterDatanodeResponseProto),
  1,
  hadoop__hdfs__register_datanode_response_proto__field_descriptors,
  hadoop__hdfs__register_datanode_response_proto__field_indices_by_name,
  1,  hadoop__hdfs__register_datanode_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__register_datanode_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const uint32_t hadoop__hdfs__heartbeat_request_proto__xmits_in_progress__default_value = 0;
static const uint32_t hadoop__hdfs__heartbeat_request_proto__xceiver_count__default_value = 0;
static const uint32_t hadoop__hdfs__heartbeat_request_proto__failed_volumes__default_value = 0;
static const ProtobufCFieldDescriptor hadoop__hdfs__heartbeat_request_proto__field_descriptors[5] =
{
  {
    "registration",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, registration),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "reports",
    2,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, n_reports),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, reports),
    &hadoop__hdfs__storage_report_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "xmitsInProgress",
    3,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT32,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, has_xmitsinprogress),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, xmitsinprogress),
    NULL,
    &hadoop__hdfs__heartbeat_request_proto__xmits_in_progress__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "xceiverCount",
    4,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT32,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, has_xceivercount),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, xceivercount),
    NULL,
    &hadoop__hdfs__heartbeat_request_proto__xceiver_count__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "failedVolumes",
    5,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT32,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, has_failedvolumes),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatRequestProto, failedvolumes),
    NULL,
    &hadoop__hdfs__heartbeat_request_proto__failed_volumes__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__heartbeat_request_proto__field_indices_by_name[] = {
  4,   /* field[4] = failedVolumes */
  0,   /* field[0] = registration */
  1,   /* field[1] = reports */
  3,   /* field[3] = xceiverCount */
  2,   /* field[2] = xmitsInProgress */
};
static const ProtobufCIntRange hadoop__hdfs__heartbeat_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 5 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__heartbeat_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.HeartbeatRequestProto",
  "HeartbeatRequestProto",
  "Hadoop__Hdfs__HeartbeatRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__HeartbeatRequestProto),
  5,
  hadoop__hdfs__heartbeat_request_proto__field_descriptors,
  hadoop__hdfs__heartbeat_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__heartbeat_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__heartbeat_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const protobuf_c_boolean hadoop__hdfs__storage_report_proto__failed__default_value = 0;
static const uint64_t hadoop__hdfs__storage_report_proto__capacity__default_value = 0;
static const uint64_t hadoop__hdfs__storage_report_proto__dfs_used__default_value = 0;
static const uint64_t hadoop__hdfs__storage_report_proto__remaining__default_value = 0;
static const uint64_t hadoop__hdfs__storage_report_proto__block_pool_used__default_value = 0;
static const ProtobufCFieldDescriptor hadoop__hdfs__storage_report_proto__field_descriptors[6] =
{
  {
    "storageID",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, storageid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "failed",
    2,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_BOOL,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, has_failed),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, failed),
    NULL,
    &hadoop__hdfs__storage_report_proto__failed__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "capacity",
    3,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT64,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, has_capacity),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, capacity),
    NULL,
    &hadoop__hdfs__storage_report_proto__capacity__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "dfsUsed",
    4,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT64,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, has_dfsused),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, dfsused),
    NULL,
    &hadoop__hdfs__storage_report_proto__dfs_used__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "remaining",
    5,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT64,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, has_remaining),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, remaining),
    NULL,
    &hadoop__hdfs__storage_report_proto__remaining__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blockPoolUsed",
    6,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_UINT64,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, has_blockpoolused),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReportProto, blockpoolused),
    NULL,
    &hadoop__hdfs__storage_report_proto__block_pool_used__default_value,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__storage_report_proto__field_indices_by_name[] = {
  5,   /* field[5] = blockPoolUsed */
  2,   /* field[2] = capacity */
  3,   /* field[3] = dfsUsed */
  1,   /* field[1] = failed */
  4,   /* field[4] = remaining */
  0,   /* field[0] = storageID */
};
static const ProtobufCIntRange hadoop__hdfs__storage_report_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 6 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__storage_report_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.StorageReportProto",
  "StorageReportProto",
  "Hadoop__Hdfs__StorageReportProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__StorageReportProto),
  6,
  hadoop__hdfs__storage_report_proto__field_descriptors,
  hadoop__hdfs__storage_report_proto__field_indices_by_name,
  1,  hadoop__hdfs__storage_report_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__storage_report_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__nnhastatus_heartbeat_proto__state__enum_values_by_number[2] =
{
  { "ACTIVE", "HADOOP__HDFS__NNHASTATUS_HEARTBEAT_PROTO__STATE__ACTIVE", 0 },
  { "STANDBY", "HADOOP__HDFS__NNHASTATUS_HEARTBEAT_PROTO__STATE__STANDBY", 1 },
};
static const ProtobufCIntRange hadoop__hdfs__nnhastatus_heartbeat_proto__state__value_ranges[] = {
{0, 0},{0, 2}
};
const ProtobufCEnumValueIndex hadoop__hdfs__nnhastatus_heartbeat_proto__state__enum_values_by_name[2] =
{
  { "ACTIVE", 0 },
  { "STANDBY", 1 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__nnhastatus_heartbeat_proto__state__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.NNHAStatusHeartbeatProto.State",
  "State",
  "Hadoop__Hdfs__NNHAStatusHeartbeatProto__State",
  "hadoop.hdfs",
  2,
  hadoop__hdfs__nnhastatus_heartbeat_proto__state__enum_values_by_number,
  2,
  hadoop__hdfs__nnhastatus_heartbeat_proto__state__enum_values_by_name,
  1,
  hadoop__hdfs__nnhastatus_heartbeat_proto__state__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__nnhastatus_heartbeat_proto__field_descriptors[2] =
{
  {
    "state",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_ENUM,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__NNHAStatusHeartbeatProto, state),
    &hadoop__hdfs__nnhastatus_heartbeat_proto__state__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "txid",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_UINT64,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__NNHAStatusHeartbeatProto, txid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__nnhastatus_heartbeat_proto__field_indices_by_name[] = {
  0,   /* field[0] = state */
  1,   /* field[1] = txid */
};
static const ProtobufCIntRange hadoop__hdfs__nnhastatus_heartbeat_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.NNHAStatusHeartbeatProto",
  "NNHAStatusHeartbeatProto",
  "Hadoop__Hdfs__NNHAStatusHeartbeatProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__NNHAStatusHeartbeatProto),
  2,
  hadoop__hdfs__nnhastatus_heartbeat_proto__field_descriptors,
  hadoop__hdfs__nnhastatus_heartbeat_proto__field_indices_by_name,
  1,  hadoop__hdfs__nnhastatus_heartbeat_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__nnhastatus_heartbeat_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__heartbeat_response_proto__field_descriptors[2] =
{
  {
    "cmds",
    1,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatResponseProto, n_cmds),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatResponseProto, cmds),
    &hadoop__hdfs__datanode_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "haStatus",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__HeartbeatResponseProto, hastatus),
    &hadoop__hdfs__nnhastatus_heartbeat_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__heartbeat_response_proto__field_indices_by_name[] = {
  0,   /* field[0] = cmds */
  1,   /* field[1] = haStatus */
};
static const ProtobufCIntRange hadoop__hdfs__heartbeat_response_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__heartbeat_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.HeartbeatResponseProto",
  "HeartbeatResponseProto",
  "Hadoop__Hdfs__HeartbeatResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__HeartbeatResponseProto),
  2,
  hadoop__hdfs__heartbeat_response_proto__field_descriptors,
  hadoop__hdfs__heartbeat_response_proto__field_indices_by_name,
  1,  hadoop__hdfs__heartbeat_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__heartbeat_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__block_report_request_proto__field_descriptors[3] =
{
  {
    "registration",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReportRequestProto, registration),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blockPoolId",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReportRequestProto, blockpoolid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "reports",
    3,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReportRequestProto, n_reports),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReportRequestProto, reports),
    &hadoop__hdfs__storage_block_report_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__block_report_request_proto__field_indices_by_name[] = {
  1,   /* field[1] = blockPoolId */
  0,   /* field[0] = registration */
  2,   /* field[2] = reports */
};
static const ProtobufCIntRange hadoop__hdfs__block_report_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 3 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__block_report_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockReportRequestProto",
  "BlockReportRequestProto",
  "Hadoop__Hdfs__BlockReportRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockReportRequestProto),
  3,
  hadoop__hdfs__block_report_request_proto__field_descriptors,
  hadoop__hdfs__block_report_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__block_report_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_report_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__storage_block_report_proto__field_descriptors[2] =
{
  {
    "storage",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageBlockReportProto, storage),
    &hadoop__hdfs__datanode_storage_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blocks",
    2,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_UINT64,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageBlockReportProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageBlockReportProto, blocks),
    NULL,
    NULL,
    1,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__storage_block_report_proto__field_indices_by_name[] = {
  1,   /* field[1] = blocks */
  0,   /* field[0] = storage */
};
static const ProtobufCIntRange hadoop__hdfs__storage_block_report_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__storage_block_report_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.StorageBlockReportProto",
  "StorageBlockReportProto",
  "Hadoop__Hdfs__StorageBlockReportProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__StorageBlockReportProto),
  2,
  hadoop__hdfs__storage_block_report_proto__field_descriptors,
  hadoop__hdfs__storage_block_report_proto__field_indices_by_name,
  1,  hadoop__hdfs__storage_block_report_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__storage_block_report_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__block_report_response_proto__field_descriptors[1] =
{
  {
    "cmd",
    1,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReportResponseProto, cmd),
    &hadoop__hdfs__datanode_command_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__block_report_response_proto__field_indices_by_name[] = {
  0,   /* field[0] = cmd */
};
static const ProtobufCIntRange hadoop__hdfs__block_report_response_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__block_report_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockReportResponseProto",
  "BlockReportResponseProto",
  "Hadoop__Hdfs__BlockReportResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockReportResponseProto),
  1,
  hadoop__hdfs__block_report_response_proto__field_descriptors,
  hadoop__hdfs__block_report_response_proto__field_indices_by_name,
  1,  hadoop__hdfs__block_report_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_report_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__received_deleted_block_info_proto__block_status__enum_values_by_number[3] =
{
  { "RECEIVING", "HADOOP__HDFS__RECEIVED_DELETED_BLOCK_INFO_PROTO__BLOCK_STATUS__RECEIVING", 1 },
  { "RECEIVED", "HADOOP__HDFS__RECEIVED_DELETED_BLOCK_INFO_PROTO__BLOCK_STATUS__RECEIVED", 2 },
  { "DELETED", "HADOOP__HDFS__RECEIVED_DELETED_BLOCK_INFO_PROTO__BLOCK_STATUS__DELETED", 3 },
};
static const ProtobufCIntRange hadoop__hdfs__received_deleted_block_info_proto__block_status__value_ranges[] = {
{1, 0},{0, 3}
};
const ProtobufCEnumValueIndex hadoop__hdfs__received_deleted_block_info_proto__block_status__enum_values_by_name[3] =
{
  { "DELETED", 2 },
  { "RECEIVED", 1 },
  { "RECEIVING", 0 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__received_deleted_block_info_proto__block_status__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ReceivedDeletedBlockInfoProto.BlockStatus",
  "BlockStatus",
  "Hadoop__Hdfs__ReceivedDeletedBlockInfoProto__BlockStatus",
  "hadoop.hdfs",
  3,
  hadoop__hdfs__received_deleted_block_info_proto__block_status__enum_values_by_number,
  3,
  hadoop__hdfs__received_deleted_block_info_proto__block_status__enum_values_by_name,
  1,
  hadoop__hdfs__received_deleted_block_info_proto__block_status__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__received_deleted_block_info_proto__field_descriptors[3] =
{
  {
    "block",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ReceivedDeletedBlockInfoProto, block),
    &hadoop__hdfs__block_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "deleteHint",
    2,
    PROTOBUF_C_LABEL_OPTIONAL,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ReceivedDeletedBlockInfoProto, deletehint),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "status",
    3,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_ENUM,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ReceivedDeletedBlockInfoProto, status),
    &hadoop__hdfs__received_deleted_block_info_proto__block_status__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__received_deleted_block_info_proto__field_indices_by_name[] = {
  0,   /* field[0] = block */
  1,   /* field[1] = deleteHint */
  2,   /* field[2] = status */
};
static const ProtobufCIntRange hadoop__hdfs__received_deleted_block_info_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 3 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__received_deleted_block_info_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ReceivedDeletedBlockInfoProto",
  "ReceivedDeletedBlockInfoProto",
  "Hadoop__Hdfs__ReceivedDeletedBlockInfoProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__ReceivedDeletedBlockInfoProto),
  3,
  hadoop__hdfs__received_deleted_block_info_proto__field_descriptors,
  hadoop__hdfs__received_deleted_block_info_proto__field_indices_by_name,
  1,  hadoop__hdfs__received_deleted_block_info_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__received_deleted_block_info_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__storage_received_deleted_blocks_proto__field_descriptors[2] =
{
  {
    "storageID",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReceivedDeletedBlocksProto, storageid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blocks",
    2,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReceivedDeletedBlocksProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__StorageReceivedDeletedBlocksProto, blocks),
    &hadoop__hdfs__received_deleted_block_info_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__storage_received_deleted_blocks_proto__field_indices_by_name[] = {
  1,   /* field[1] = blocks */
  0,   /* field[0] = storageID */
};
static const ProtobufCIntRange hadoop__hdfs__storage_received_deleted_blocks_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 2 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.StorageReceivedDeletedBlocksProto",
  "StorageReceivedDeletedBlocksProto",
  "Hadoop__Hdfs__StorageReceivedDeletedBlocksProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__StorageReceivedDeletedBlocksProto),
  2,
  hadoop__hdfs__storage_received_deleted_blocks_proto__field_descriptors,
  hadoop__hdfs__storage_received_deleted_blocks_proto__field_indices_by_name,
  1,  hadoop__hdfs__storage_received_deleted_blocks_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__storage_received_deleted_blocks_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__block_received_and_deleted_request_proto__field_descriptors[3] =
{
  {
    "registration",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto, registration),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blockPoolId",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto, blockpoolid),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "blocks",
    3,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto, blocks),
    &hadoop__hdfs__storage_received_deleted_blocks_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__block_received_and_deleted_request_proto__field_indices_by_name[] = {
  1,   /* field[1] = blockPoolId */
  2,   /* field[2] = blocks */
  0,   /* field[0] = registration */
};
static const ProtobufCIntRange hadoop__hdfs__block_received_and_deleted_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 3 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__block_received_and_deleted_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockReceivedAndDeletedRequestProto",
  "BlockReceivedAndDeletedRequestProto",
  "Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto),
  3,
  hadoop__hdfs__block_received_and_deleted_request_proto__field_descriptors,
  hadoop__hdfs__block_received_and_deleted_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__block_received_and_deleted_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_received_and_deleted_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
#define hadoop__hdfs__block_received_and_deleted_response_proto__field_descriptors NULL
#define hadoop__hdfs__block_received_and_deleted_response_proto__field_indices_by_name NULL
#define hadoop__hdfs__block_received_and_deleted_response_proto__number_ranges NULL
const ProtobufCMessageDescriptor hadoop__hdfs__block_received_and_deleted_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.BlockReceivedAndDeletedResponseProto",
  "BlockReceivedAndDeletedResponseProto",
  "Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto),
  0,
  hadoop__hdfs__block_received_and_deleted_response_proto__field_descriptors,
  hadoop__hdfs__block_received_and_deleted_response_proto__field_indices_by_name,
  0,  hadoop__hdfs__block_received_and_deleted_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__block_received_and_deleted_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
const ProtobufCEnumValue hadoop__hdfs__error_report_request_proto__error_code__enum_values_by_number[4] =
{
  { "NOTIFY", "HADOOP__HDFS__ERROR_REPORT_REQUEST_PROTO__ERROR_CODE__NOTIFY", 0 },
  { "DISK_ERROR", "HADOOP__HDFS__ERROR_REPORT_REQUEST_PROTO__ERROR_CODE__DISK_ERROR", 1 },
  { "INVALID_BLOCK", "HADOOP__HDFS__ERROR_REPORT_REQUEST_PROTO__ERROR_CODE__INVALID_BLOCK", 2 },
  { "FATAL_DISK_ERROR", "HADOOP__HDFS__ERROR_REPORT_REQUEST_PROTO__ERROR_CODE__FATAL_DISK_ERROR", 3 },
};
static const ProtobufCIntRange hadoop__hdfs__error_report_request_proto__error_code__value_ranges[] = {
{0, 0},{0, 4}
};
const ProtobufCEnumValueIndex hadoop__hdfs__error_report_request_proto__error_code__enum_values_by_name[4] =
{
  { "DISK_ERROR", 1 },
  { "FATAL_DISK_ERROR", 3 },
  { "INVALID_BLOCK", 2 },
  { "NOTIFY", 0 },
};
const ProtobufCEnumDescriptor hadoop__hdfs__error_report_request_proto__error_code__descriptor =
{
  PROTOBUF_C_ENUM_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ErrorReportRequestProto.ErrorCode",
  "ErrorCode",
  "Hadoop__Hdfs__ErrorReportRequestProto__ErrorCode",
  "hadoop.hdfs",
  4,
  hadoop__hdfs__error_report_request_proto__error_code__enum_values_by_number,
  4,
  hadoop__hdfs__error_report_request_proto__error_code__enum_values_by_name,
  1,
  hadoop__hdfs__error_report_request_proto__error_code__value_ranges,
  NULL,NULL,NULL,NULL   /* reserved[1234] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__error_report_request_proto__field_descriptors[3] =
{
  {
    "registartion",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ErrorReportRequestProto, registartion),
    &hadoop__hdfs__datanode_registration_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "errorCode",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_UINT32,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ErrorReportRequestProto, errorcode),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "msg",
    3,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_STRING,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ErrorReportRequestProto, msg),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__error_report_request_proto__field_indices_by_name[] = {
  1,   /* field[1] = errorCode */
  2,   /* field[2] = msg */
  0,   /* field[0] = registartion */
};
static const ProtobufCIntRange hadoop__hdfs__error_report_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 3 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__error_report_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ErrorReportRequestProto",
  "ErrorReportRequestProto",
  "Hadoop__Hdfs__ErrorReportRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__ErrorReportRequestProto),
  3,
  hadoop__hdfs__error_report_request_proto__field_descriptors,
  hadoop__hdfs__error_report_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__error_report_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__error_report_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
#define hadoop__hdfs__error_report_response_proto__field_descriptors NULL
#define hadoop__hdfs__error_report_response_proto__field_indices_by_name NULL
#define hadoop__hdfs__error_report_response_proto__number_ranges NULL
const ProtobufCMessageDescriptor hadoop__hdfs__error_report_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ErrorReportResponseProto",
  "ErrorReportResponseProto",
  "Hadoop__Hdfs__ErrorReportResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__ErrorReportResponseProto),
  0,
  hadoop__hdfs__error_report_response_proto__field_descriptors,
  hadoop__hdfs__error_report_response_proto__field_indices_by_name,
  0,  hadoop__hdfs__error_report_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__error_report_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__report_bad_blocks_request_proto__field_descriptors[1] =
{
  {
    "blocks",
    1,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ReportBadBlocksRequestProto, n_blocks),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__ReportBadBlocksRequestProto, blocks),
    &hadoop__hdfs__located_block_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__report_bad_blocks_request_proto__field_indices_by_name[] = {
  0,   /* field[0] = blocks */
};
static const ProtobufCIntRange hadoop__hdfs__report_bad_blocks_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 1 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__report_bad_blocks_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ReportBadBlocksRequestProto",
  "ReportBadBlocksRequestProto",
  "Hadoop__Hdfs__ReportBadBlocksRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__ReportBadBlocksRequestProto),
  1,
  hadoop__hdfs__report_bad_blocks_request_proto__field_descriptors,
  hadoop__hdfs__report_bad_blocks_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__report_bad_blocks_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__report_bad_blocks_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
#define hadoop__hdfs__report_bad_blocks_response_proto__field_descriptors NULL
#define hadoop__hdfs__report_bad_blocks_response_proto__field_indices_by_name NULL
#define hadoop__hdfs__report_bad_blocks_response_proto__number_ranges NULL
const ProtobufCMessageDescriptor hadoop__hdfs__report_bad_blocks_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.ReportBadBlocksResponseProto",
  "ReportBadBlocksResponseProto",
  "Hadoop__Hdfs__ReportBadBlocksResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__ReportBadBlocksResponseProto),
  0,
  hadoop__hdfs__report_bad_blocks_response_proto__field_descriptors,
  hadoop__hdfs__report_bad_blocks_response_proto__field_indices_by_name,
  0,  hadoop__hdfs__report_bad_blocks_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__report_bad_blocks_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCFieldDescriptor hadoop__hdfs__commit_block_synchronization_request_proto__field_descriptors[7] =
{
  {
    "block",
    1,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_MESSAGE,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, block),
    &hadoop__hdfs__extended_block_proto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "newGenStamp",
    2,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_UINT64,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, newgenstamp),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "newLength",
    3,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_UINT64,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, newlength),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "closeFile",
    4,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_BOOL,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, closefile),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "deleteBlock",
    5,
    PROTOBUF_C_LABEL_REQUIRED,
    PROTOBUF_C_TYPE_BOOL,
    0,   /* quantifier_offset */
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, deleteblock),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "newTaragets",
    6,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_MESSAGE,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, n_newtaragets),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, newtaragets),
    &hadoop__hdfs__datanode_idproto__descriptor,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
  {
    "newTargetStorages",
    7,
    PROTOBUF_C_LABEL_REPEATED,
    PROTOBUF_C_TYPE_STRING,
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, n_newtargetstorages),
    PROTOBUF_C_OFFSETOF(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto, newtargetstorages),
    NULL,
    NULL,
    0,            /* packed */
    0,NULL,NULL    /* reserved1,reserved2, etc */
  },
};
static const unsigned hadoop__hdfs__commit_block_synchronization_request_proto__field_indices_by_name[] = {
  0,   /* field[0] = block */
  3,   /* field[3] = closeFile */
  4,   /* field[4] = deleteBlock */
  1,   /* field[1] = newGenStamp */
  2,   /* field[2] = newLength */
  5,   /* field[5] = newTaragets */
  6,   /* field[6] = newTargetStorages */
};
static const ProtobufCIntRange hadoop__hdfs__commit_block_synchronization_request_proto__number_ranges[1 + 1] =
{
  { 1, 0 },
  { 0, 7 }
};
const ProtobufCMessageDescriptor hadoop__hdfs__commit_block_synchronization_request_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.CommitBlockSynchronizationRequestProto",
  "CommitBlockSynchronizationRequestProto",
  "Hadoop__Hdfs__CommitBlockSynchronizationRequestProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__CommitBlockSynchronizationRequestProto),
  7,
  hadoop__hdfs__commit_block_synchronization_request_proto__field_descriptors,
  hadoop__hdfs__commit_block_synchronization_request_proto__field_indices_by_name,
  1,  hadoop__hdfs__commit_block_synchronization_request_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__commit_block_synchronization_request_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
#define hadoop__hdfs__commit_block_synchronization_response_proto__field_descriptors NULL
#define hadoop__hdfs__commit_block_synchronization_response_proto__field_indices_by_name NULL
#define hadoop__hdfs__commit_block_synchronization_response_proto__number_ranges NULL
const ProtobufCMessageDescriptor hadoop__hdfs__commit_block_synchronization_response_proto__descriptor =
{
  PROTOBUF_C_MESSAGE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.CommitBlockSynchronizationResponseProto",
  "CommitBlockSynchronizationResponseProto",
  "Hadoop__Hdfs__CommitBlockSynchronizationResponseProto",
  "hadoop.hdfs",
  sizeof(Hadoop__Hdfs__CommitBlockSynchronizationResponseProto),
  0,
  hadoop__hdfs__commit_block_synchronization_response_proto__field_descriptors,
  hadoop__hdfs__commit_block_synchronization_response_proto__field_indices_by_name,
  0,  hadoop__hdfs__commit_block_synchronization_response_proto__number_ranges,
  (ProtobufCMessageInit) hadoop__hdfs__commit_block_synchronization_response_proto__init,
  NULL,NULL,NULL    /* reserved[123] */
};
static const ProtobufCMethodDescriptor hadoop__hdfs__datanode_protocol_service__method_descriptors[8] =
{
  { "registerDatanode", &hadoop__hdfs__register_datanode_request_proto__descriptor, &hadoop__hdfs__register_datanode_response_proto__descriptor },
  { "sendHeartbeat", &hadoop__hdfs__heartbeat_request_proto__descriptor, &hadoop__hdfs__heartbeat_response_proto__descriptor },
  { "blockReport", &hadoop__hdfs__block_report_request_proto__descriptor, &hadoop__hdfs__block_report_response_proto__descriptor },
  { "blockReceivedAndDeleted", &hadoop__hdfs__block_received_and_deleted_request_proto__descriptor, &hadoop__hdfs__block_received_and_deleted_response_proto__descriptor },
  { "errorReport", &hadoop__hdfs__error_report_request_proto__descriptor, &hadoop__hdfs__error_report_response_proto__descriptor },
  { "versionRequest", &hadoop__hdfs__version_request_proto__descriptor, &hadoop__hdfs__version_response_proto__descriptor },
  { "reportBadBlocks", &hadoop__hdfs__report_bad_blocks_request_proto__descriptor, &hadoop__hdfs__report_bad_blocks_response_proto__descriptor },
  { "commitBlockSynchronization", &hadoop__hdfs__commit_block_synchronization_request_proto__descriptor, &hadoop__hdfs__commit_block_synchronization_response_proto__descriptor },
};
const unsigned hadoop__hdfs__datanode_protocol_service__method_indices_by_name[] = {
  3,        /* blockReceivedAndDeleted */
  2,        /* blockReport */
  7,        /* commitBlockSynchronization */
  4,        /* errorReport */
  0,        /* registerDatanode */
  6,        /* reportBadBlocks */
  1,        /* sendHeartbeat */
  5         /* versionRequest */
};
const ProtobufCServiceDescriptor hadoop__hdfs__datanode_protocol_service__descriptor =
{
  PROTOBUF_C_SERVICE_DESCRIPTOR_MAGIC,
  "hadoop.hdfs.DatanodeProtocolService",
  "versionRequest",
  "Hadoop__Hdfs__DatanodeProtocolService",
  "hadoop.hdfs",
  8,
  hadoop__hdfs__datanode_protocol_service__method_descriptors,
  hadoop__hdfs__datanode_protocol_service__method_indices_by_name
};
void hadoop__hdfs__datanode_protocol_service__register_datanode(ProtobufCService *service,
                                                                const Hadoop__Hdfs__RegisterDatanodeRequestProto *input,
                                                                Hadoop__Hdfs__RegisterDatanodeResponseProto_Closure closure,
                                                                void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 0, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__send_heartbeat(ProtobufCService *service,
                                                             const Hadoop__Hdfs__HeartbeatRequestProto *input,
                                                             Hadoop__Hdfs__HeartbeatResponseProto_Closure closure,
                                                             void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 1, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__block_report(ProtobufCService *service,
                                                           const Hadoop__Hdfs__BlockReportRequestProto *input,
                                                           Hadoop__Hdfs__BlockReportResponseProto_Closure closure,
                                                           void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 2, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__block_received_and_deleted(ProtobufCService *service,
                                                                         const Hadoop__Hdfs__BlockReceivedAndDeletedRequestProto *input,
                                                                         Hadoop__Hdfs__BlockReceivedAndDeletedResponseProto_Closure closure,
                                                                         void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 3, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__error_report(ProtobufCService *service,
                                                           const Hadoop__Hdfs__ErrorReportRequestProto *input,
                                                           Hadoop__Hdfs__ErrorReportResponseProto_Closure closure,
                                                           void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 4, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__version_request(ProtobufCService *service,
                                                              const Hadoop__Hdfs__VersionRequestProto *input,
                                                              Hadoop__Hdfs__VersionResponseProto_Closure closure,
                                                              void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 5, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__report_bad_blocks(ProtobufCService *service,
                                                                const Hadoop__Hdfs__ReportBadBlocksRequestProto *input,
                                                                Hadoop__Hdfs__ReportBadBlocksResponseProto_Closure closure,
                                                                void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 6, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__commit_block_synchronization(ProtobufCService *service,
                                                                           const Hadoop__Hdfs__CommitBlockSynchronizationRequestProto *input,
                                                                           Hadoop__Hdfs__CommitBlockSynchronizationResponseProto_Closure closure,
                                                                           void *closure_data)
{
  PROTOBUF_C_ASSERT (service->descriptor == &hadoop__hdfs__datanode_protocol_service__descriptor);
  service->invoke(service, 7, (const ProtobufCMessage *) input, (ProtobufCClosure) closure, closure_data);
}
void hadoop__hdfs__datanode_protocol_service__init (Hadoop__Hdfs__DatanodeProtocolService_Service *service,
                                                    Hadoop__Hdfs__DatanodeProtocolService_ServiceDestroy destroy)
{
  protobuf_c_service_generated_init (&service->base,
                                     &hadoop__hdfs__datanode_protocol_service__descriptor,
                                     (ProtobufCServiceDestroy) destroy);
}
